{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Sense42 Raw EEG Data\n",
    "Load and inspect the parquet EEG files and their associated label `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/sense42_raw_eeg_extracted_rawonly\"\n",
    "\n",
    "# List all files\n",
    "all_files = sorted(os.listdir(DATA_DIR))\n",
    "parquet_files = [f for f in all_files if f.endswith(\".parquet\")]\n",
    "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
    "json_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"  .parquet : {len(parquet_files)}\")\n",
    "print(f\"  .txt     : {len(txt_files)}\")\n",
    "print(f\"  .json    : {len(json_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse filenames to understand the dataset structure\n",
    "participants = set()\n",
    "segments_per_participant = defaultdict(set)\n",
    "label_types = set()\n",
    "\n",
    "for f in all_files:\n",
    "    if not f.startswith(\"P\"):\n",
    "        continue\n",
    "    parts = f.split(\"_seg\")\n",
    "    pid = parts[0]  # e.g. P001\n",
    "    participants.add(pid)\n",
    "\n",
    "    seg_rest = parts[1]  # e.g. 04_eeg_raw_mental.txt\n",
    "    seg_id = seg_rest.split(\"_\")[0]  # e.g. 04\n",
    "    segments_per_participant[pid].add(seg_id)\n",
    "\n",
    "    # Extract label type\n",
    "    basename = os.path.splitext(f)[0]  # strip extension\n",
    "    after_raw = basename.split(\"eeg_raw\")[-1]  # e.g. \"_mental\" or \"\"\n",
    "    label = after_raw.lstrip(\"_\") if after_raw else \"raw\"\n",
    "    label_types.add(label if label else \"raw\")\n",
    "\n",
    "print(f\"Participants: {len(participants)} ({min(participants)} .. {max(participants)})\")\n",
    "seg_counts = [len(v) for v in segments_per_participant.values()]\n",
    "print(f\"Segments per participant: min={min(seg_counts)}, max={max(seg_counts)}, mean={np.mean(seg_counts):.1f}\")\n",
    "print(f\"Label types: {sorted(label_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Single Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first parquet file as an example\n",
    "sample_parquet = parquet_files[0]\n",
    "sample_path = os.path.join(DATA_DIR, sample_parquet)\n",
    "print(f\"Loading: {sample_parquet}\")\n",
    "print(f\"File size: {os.path.getsize(sample_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# --- Method A: Read with pyarrow (fastest, lowest memory) ---\n",
    "try:\n",
    "    pf = pq.ParquetFile(sample_path)\n",
    "    print(f\"\\n--- PyArrow ParquetFile metadata ---\")\n",
    "    print(f\"Schema: {pf.schema_arrow}\")\n",
    "    print(f\"Num columns: {pf.schema_arrow.num_fields if hasattr(pf.schema_arrow, 'num_fields') else 'N/A'}\")\n",
    "    print(f\"Num row groups: {pf.metadata.num_row_groups}\")\n",
    "    print(f\"Num rows: {pf.metadata.num_rows}\")\n",
    "except Exception as e:\n",
    "    print(f\"PyArrow read failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method B: Read into pandas DataFrame ---\n",
    "try:\n",
    "    df = pd.read_parquet(sample_path)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Dtypes:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "    display(df.head())\n",
    "    display(df.describe())\n",
    "except Exception as e:\n",
    "    print(f\"Pandas read failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method C: Convert to numpy array ---\n",
    "try:\n",
    "    df = pd.read_parquet(sample_path)\n",
    "    eeg_array = df.to_numpy(dtype=np.float64)\n",
    "    print(f\"Numpy array shape: {eeg_array.shape}\")\n",
    "    print(f\"Dtype: {eeg_array.dtype}\")\n",
    "    print(f\"Min: {np.nanmin(eeg_array):.6f}, Max: {np.nanmax(eeg_array):.6f}, Mean: {np.nanmean(eeg_array):.6f}\")\n",
    "    print(f\"Any NaNs: {np.isnan(eeg_array).any()}\")\n",
    "    print(f\"\\nFirst 3 rows x first 5 cols:\\n{eeg_array[:3, :5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Numpy conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Corresponding Label Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\"effort\", \"frustration\", \"mental\", \"performance\", \"temporal\"]\n",
    "\n",
    "def load_label(filepath):\n",
    "    \"\"\"Try multiple strategies to read a label value from a small binary/text file.\"\"\"\n",
    "    size = os.path.getsize(filepath)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # Strategy 1: try reading as plain text float\n",
    "    try:\n",
    "        return float(raw.strip())\n",
    "    except (ValueError, UnicodeDecodeError):\n",
    "        pass\n",
    "\n",
    "    # Strategy 2: try interpreting as a packed float32\n",
    "    if len(raw) == 4:\n",
    "        val = np.frombuffer(raw, dtype=np.float32)\n",
    "        return float(val[0])\n",
    "\n",
    "    # Strategy 3: try interpreting as a packed float64\n",
    "    if len(raw) == 8:\n",
    "        val = np.frombuffer(raw, dtype=np.float64)\n",
    "        return float(val[0])\n",
    "\n",
    "    # Strategy 4: try interpreting as packed int32\n",
    "    if len(raw) == 4:\n",
    "        val = np.frombuffer(raw, dtype=np.int32)\n",
    "        return int(val[0])\n",
    "\n",
    "    return raw  # return raw bytes if nothing works\n",
    "\n",
    "\n",
    "# Load labels for the same segment as the sample parquet\n",
    "# e.g. P001_seg01_eeg_raw.parquet -> P001_seg01\n",
    "seg_prefix = sample_parquet.replace(\"_eeg_raw.parquet\", \"\")\n",
    "print(f\"Segment: {seg_prefix}\")\n",
    "print()\n",
    "\n",
    "for label in LABEL_NAMES:\n",
    "    # Try .txt first\n",
    "    label_file = os.path.join(DATA_DIR, f\"{seg_prefix}_eeg_raw_{label}.txt\")\n",
    "    if os.path.exists(label_file):\n",
    "        val = load_label(label_file)\n",
    "        print(f\"  {label:15s} = {val}  (size: {os.path.getsize(label_file)} bytes)\")\n",
    "    else:\n",
    "        print(f\"  {label:15s} : FILE NOT FOUND\")\n",
    "\n",
    "# Also the base raw label\n",
    "base_label = os.path.join(DATA_DIR, f\"{seg_prefix}_eeg_raw.txt\")\n",
    "if os.path.exists(base_label):\n",
    "    val = load_label(base_label)\n",
    "    print(f\"  {'raw':15s} = {val}  (size: {os.path.getsize(base_label)} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch-Load All Parquet Files into a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"Parse a parquet filename into (participant_id, segment_id).\"\"\"\n",
    "    # e.g. P001_seg04_eeg_raw.parquet\n",
    "    base = filename.replace(\"_eeg_raw.parquet\", \"\")\n",
    "    parts = base.split(\"_seg\")\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "# Load a subset (first N parquet files) to avoid memory issues\n",
    "N = 5  # increase this to load more\n",
    "dataset = {}\n",
    "\n",
    "for pf_name in parquet_files[:N]:\n",
    "    pid, seg = parse_filename(pf_name)\n",
    "    path = os.path.join(DATA_DIR, pf_name)\n",
    "    seg_prefix = pf_name.replace(\"_eeg_raw.parquet\", \"\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        eeg = df.to_numpy(dtype=np.float64)\n",
    "    except Exception as e:\n",
    "        print(f\"  SKIP {pf_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load labels\n",
    "    labels = {}\n",
    "    for label in LABEL_NAMES:\n",
    "        lpath = os.path.join(DATA_DIR, f\"{seg_prefix}_eeg_raw_{label}.txt\")\n",
    "        if os.path.exists(lpath):\n",
    "            labels[label] = load_label(lpath)\n",
    "\n",
    "    dataset[(pid, seg)] = {\n",
    "        \"eeg\": eeg,\n",
    "        \"labels\": labels,\n",
    "        \"columns\": list(df.columns),\n",
    "    }\n",
    "    print(f\"Loaded {pf_name}: shape={eeg.shape}, labels={labels}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(dataset)} segments into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Data Summary Across All Parquet Files (metadata only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan parquet metadata without loading data into memory\n",
    "summary_rows = []\n",
    "\n",
    "for pf_name in parquet_files:\n",
    "    path = os.path.join(DATA_DIR, pf_name)\n",
    "    pid, seg = parse_filename(pf_name)\n",
    "    try:\n",
    "        meta = pq.read_metadata(path)\n",
    "        schema = pq.read_schema(path)\n",
    "        summary_rows.append({\n",
    "            \"participant\": pid,\n",
    "            \"segment\": seg,\n",
    "            \"num_rows\": meta.num_rows,\n",
    "            \"num_cols\": schema.num_fields if hasattr(schema, 'num_fields') else len(schema),\n",
    "            \"file_size_mb\": os.path.getsize(path) / 1024 / 1024,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        summary_rows.append({\n",
    "            \"participant\": pid,\n",
    "            \"segment\": seg,\n",
    "            \"num_rows\": None,\n",
    "            \"num_cols\": None,\n",
    "            \"file_size_mb\": os.path.getsize(path) / 1024 / 1024,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"=== Parquet File Metadata Summary ===\")\n",
    "display(summary_df.describe())\n",
    "display(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the Extraction Summary JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = os.path.join(DATA_DIR, \"extraction_summary.json\")\n",
    "if os.path.exists(json_path):\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            extraction_summary = json.load(f)\n",
    "        print(f\"Keys: {list(extraction_summary.keys()) if isinstance(extraction_summary, dict) else type(extraction_summary)}\")\n",
    "        # Pretty-print first level\n",
    "        for k, v in (extraction_summary.items() if isinstance(extraction_summary, dict) else [(\"root\", extraction_summary)]):\n",
    "            if isinstance(v, (dict, list)):\n",
    "                print(f\"  {k}: {type(v).__name__} with {len(v)} items\")\n",
    "            else:\n",
    "                print(f\"  {k}: {v}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load JSON: {e}\")\n",
    "        # Try reading raw bytes\n",
    "        with open(json_path, \"rb\") as f:\n",
    "            raw = f.read(500)\n",
    "        print(f\"Raw bytes (first 500): {raw}\")\n",
    "else:\n",
    "    print(\"extraction_summary.json not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
